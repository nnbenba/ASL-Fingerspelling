# ASL-Fingerspelling

SignSpeak is a full-stack machine learning application designed to perform real-time American Sign Language (ASL) fingerspelling recognition from images. The system uses a pre-trained convolutional neural network (CNN) to classify static images of hand signs corresponding to the 26 letters of the ASL alphabet. By separating model training from deployment, the application focuses on reliable inference, efficient preprocessing, and user accessibility. The trained Keras model is loaded at runtime and used exclusively for prediction, ensuring fast and consistent performance without requiring retraining.

The backend of the application is implemented using Flask and serves as an inference API that accepts image uploads, preprocesses them to match the model’s expected input format, and returns probabilistic predictions. Uploaded images are resized, normalized, and converted into tensors before being passed through the CNN. The model outputs a probability distribution over all possible ASL letters, from which the most likely prediction and its associated confidence score are extracted and returned to the client. This probabilistic output allows users to understand not only the predicted letter but also the model’s level of certainty.

The frontend provides an interactive web interface that allows users to upload images directly from their device. It includes live image previews, input validation, and a responsive results display that visualizes both the predicted letter and confidence score. The frontend communicates with the backend via HTTP requests, creating a clean separation between presentation and inference logic. This architecture enables easy extension to other deployment environments and demonstrates best practices for integrating machine learning models into user-facing applications.
<img width="1314" height="666" alt="Screenshot 2026-01-15 at 8 54 45 PM" src="https://github.com/user-attachments/assets/dd09d04c-34a6-4253-bc41-15ee406bb2e7" />
